# 1. code till 12 pm

- SQL
  - This will mostly be a scenario based mainly using group by, partitions, windows and rank logic
  - It can be scenario based like sales questions having 3 tables how to find out highest or second highest selling product as per category wise along with geo
- Python DSA
  - check logic building ability
- Python topics
  - important methods that is most commonly used in solving DSA problems in interview
  - top 7 + 7 + 7 + 7 (tricky code snippets testing python knowledge)
  - lambda
  - try except
  - kwargs
  - file handling
  - etc most important ones
- Pyspark
- Airflow

# 2. Akash Advice (start reading from 1pm)

- spark and its architecture and some tuning process
  - mainly focus on architecture and framework of spark
  - spark archietcture vidoe (non raja one)
  - raja engineering (topics notes)
  - data Engineering - Darshan Parmil topics based notes
- about framework and one SQL question along with one python logic building code
- lakehouse features and delta importance
- Now here everything on databricks (microsoft link + )
- This will mostly be a scenario based mainly using group by, partitions, windows and rank logic
- Airflow as a scheduler they rarely ask
- Few basic questions you can aspect from streaming
- Bronze, silver and gold level architecture
- It can be scenario based like sales questions having 3 tables how to find out highest or second highest selling product as per category wise along with geo

# 3. Cisco Project experience - (read end to end - know all points - second highest priority after)

# top 10 questions +10+10(t:30) for each topic (first focus on JD) (second on resume) - REVISE 3 TIMES

- https://chatgpt.com/c/6758de8c-50f8-8010-a635-31d223d7f3c8

## spark get top 100

## top 50 for JD keywords based topics dataengineering, dataPipeline, dataEngineerconcepts(even more---70), SQL, Database, Kafka, etc)

## later go and read top 10 +10+10 asked in FAANG interview (4th revision)

## later go and read top 10 +10+10 asked in interview for data engineer (5th revision)

# TODO List & Nike-Keywords-JD

# Pyspark hands on -> 3 people

# topics-notes-list- basic questions

# top 50 questions

# Google Keep -

# Darshan Parmil List:

```
  0:00 Introduction
  1:19 What is Data Engineering?
  17:01 Data Engineering Lifecycle
  27:17 Data Generation vs Storage
  30:20 Database Management System
  34:23 Data Modelling
  43:48 NoSQL Databases
  44:49 SQL vs NoSQL
  46:17 Storage processing (OLAP vs OLTP)
  57:26 ETL (Extract Transform Load)
  59:12 Data Engineering Undercurrents
  1:05:16 Data Architecture 101 Complete Guide
  1:27:35 Data Warehouse
  1:33:21 Dimensional Modelling
  1:40:34 Slowly Changing Dimensions
  1:47:58 Data Marts
  1:52:01 Data Lake
  1:56:30 Data Lake vs Data Warehouse
  2:01:41 Big Data Landscape
  2:03:16 Cloud Computing
  2:09:00 AWS Data Engineering Services
  2:17:33 Case Study - AWS Data Engineering
  2:25:57 GCP Data Engineering & Case Study
  2:27:07 Azure Data Engineering & Case Study
  2:32:32 Modern Data Architecture
  2:37:32 Important Skills for Data Engineering
  2:40:17 Top Data Warehouse Tools
  2:41:49 Top Data Processing Tools
  2:42:35 Data Orchestration Tools
  2:43:33 Modern Data Stack
  2:45:20 Python, SQL, DW, Spark, Airflow for Data Engineering
  2:51:40 Data Security
  2:52:46 Data Masking
  2:53:41 Important File Formats
  2:53:49 Suggestion for Part 2
  2:54:58 Data Engineering Course(14 Projects) Combo Offer
```

---

Chatgpt interview:
A 1.5-hour interview for a **Data Engineer** with 1 year of experience typically includes several segments to assess your technical, problem-solving, and communication skills. Here’s a general breakdown:

---

### **1. Introduction (5-10 minutes)**

- **Purpose**: Ice-breaking and understanding your background.
- **Topics Covered**:
  - Brief self-introduction.
  - Overview of your current or previous role(s).
  - Discussion of projects you’ve worked on, especially those related to data engineering (e.g., ETL pipelines, data modeling, or working with big data tools).

---

### **2. Technical Assessment (45-60 minutes)**

This is the main focus of the interview, where your technical expertise is evaluated.

#### **a. SQL and Data Manipulation (10-20 minutes)**

- Expect questions requiring you to write complex SQL queries. Example:
  - Joins (inner, outer, etc.).
  - Window functions (e.g., `ROW_NUMBER`, `RANK`, `OVER`).
  - Data aggregation and transformation.
  - Query optimization.

#### **b. Programming (10-15 minutes)**

- Assessment of your coding ability (often in Python or another language relevant to the job).
- Examples:
  - Writing a function to transform or process data.
  - Parsing and cleaning datasets.
  - Basic algorithms or data structures (e.g., arrays, hashmaps, sorting).

#### **c. Data Engineering Tools and Technologies (10-15 minutes)**

- Questions about tools you’ve used (e.g., Apache Kafka, Spark, Airflow, etc.).
- Example topics:
  - Designing and implementing ETL pipelines.
  - Working with databases (SQL/NoSQL).
  - Cloud services for data engineering (AWS, GCP, or Azure).

#### **d. System Design (10-15 minutes)**

- Designing a data pipeline or architecture.
- Example:
  - "Design a data pipeline to process real-time user events and store aggregated results in a database."
  - Questions about handling scalability, reliability, and fault tolerance.

---

### **3. Behavioral Questions (15-20 minutes)**

- Focus on teamwork, problem-solving, and communication.
- Example questions:
  - “Tell me about a time you optimized a data process.”
  - “How do you prioritize tasks when dealing with tight deadlines?”
  - “Describe a challenging bug or issue you encountered and how you resolved it.”

---

### **4. Q&A (5-10 minutes)**

- **Purpose**: To evaluate your interest and give you a chance to ask questions.
- Example questions you can ask:
  - "What does success look like for this role?"
  - "What are some upcoming projects the team is excited about?"
  - "What are the growth opportunities in this role?"

---

### **Tips to Succeed:**

1. **Brush up on SQL:** Be ready to write efficient and optimized queries.
2. **Be confident in your projects:** Highlight any hands-on experience with data pipelines, cloud tools, or large datasets.
3. **Prepare for coding:** Practice Python (or relevant language) with data-related problems.
4. **Review data engineering tools:** Familiarize yourself with ETL processes, data warehousing, and any tools mentioned in the job description.
5. **Focus on communication:** Clearly explain your thought process during problem-solving.

This structure may vary, but the emphasis is usually on your technical expertise and ability to handle data-related challenges.

---

Here’s a set of **30+ commonly asked programming questions** during interviews for a Data Engineer or related roles. These cover **Python**, **data manipulation**, and **coding fundamentals**.

---

### **1. Basic Programming Questions**

1. Write a program to reverse a string without using built-in functions.
2. Implement a function to check if a string is a palindrome.
3. Write a program to find the factorial of a number.
4. Implement Fibonacci sequence generation up to `n` terms.
5. Write a function to count the occurrences of each character in a string.
6. Given a list of integers, return the second largest element.

---

### **2. Data Structures**

7. Implement a stack using a list in Python.
8. Write a function to check if a given string has balanced parentheses.
9. Implement a queue using two stacks.
10. Find the intersection of two arrays.
11. Write a function to detect a cycle in a linked list.
12. Sort an array of integers using quicksort or mergesort.

---

### **3. Algorithms**

13. Write a function to find the longest common prefix among an array of strings.
14. Implement binary search to find an element in a sorted array.
15. Write a function to find the majority element in an array (appears more than n/2 times).
16. Find the maximum sum of a contiguous subarray (Kadane's Algorithm).
17. Given an array, find all pairs that sum up to a given target value.
18. Write a program to find the minimum number of steps required to reduce a number to 1 using the following operations: subtract 1, divide by 2, divide by 3.

---

### **4. Python-Specific**

19. What is the difference between a list and a tuple in Python? Provide examples.
20. Explain the usage of `lambda`, `map`, and `filter` functions with examples.
21. Write a Python program to flatten a nested list.
22. Explain Python's `with` statement and write an example using file handling.
23. Implement a decorator function in Python.
24. Write a program to read and process data from a CSV file using `pandas`.

---

### **5. Data Processing**

25. Write a function to calculate the average of each column in a CSV file.
26. Transform a list of dictionaries into a single dictionary, grouped by a common key.
27. Parse a log file and count the occurrences of each HTTP status code.
28. Implement a function to remove duplicates from a list while preserving the order.
29. Write a program to merge two sorted arrays into a single sorted array.

---

### **6. Advanced Topics**

30. Implement a simple producer-consumer model using Python's `queue` and `threading`.
31. Write a function to calculate the Levenshtein distance between two strings.
32. Create a data pipeline simulation: Read data from a JSON file, clean it, and store it in a database.
33. Implement a function to calculate the rolling average of a time series data.
34. Create a basic REST API in Python using Flask for a `users` table with CRUD operations.

---

### **7. Practical Use Cases**

35. Write a program to convert a nested JSON object into a flat dictionary.
36. Parse a large dataset, group by a specific column, and compute aggregated metrics like mean and count.
37. Write a script to detect and remove duplicate rows in a large dataset.
38. Simulate a stream of events and process them in real-time using Python (e.g., Kafka, or just a generator function).
39. Develop a function to validate if a given schema matches the structure of a JSON object.

---

These questions are frequently asked to assess problem-solving skills, proficiency in programming, and practical experience with **data engineering workflows**. Preparing for these will give you a strong foundation!
